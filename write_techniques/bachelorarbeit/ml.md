# ML -> machine learning

[Pioneers of deep ml](https://www.mckinsey.com/featured-insights/artificial-intelligence/deep-learnings-origins-and-pioneers)

* Warren McCulloch and Walter Pitts, who as early as 1943
* Bernard Widrow and Ted Hoff at Stanford University
* Frank Rosenblatt, an American psychologist
* MIT’s Marvin Minsky and Seymour Papert then put a damper on this research in their 1969 book Perceptrons
* Geoffrey Hinton at the University of Toronto, along with colleagues David Rumelhart and Ronald Williams, solved this training problem with the publication of a now famous back-propagation
* although some practitioners point to a Finnish mathematician, Seppo Linnainmaa, as having invented back-propagation already in the 1960s
* Yann LeCun at New York University pioneered the use of neural networks on image-recognition tasks and his 1998 paper
* John Hopfield popularized the “Hopfield” network 
* Jürgen Schmidhuber and Sepp Hochreiter in 1997 with the introduction of the long short-term memory (LSTM)
* Hinton and two of his students in 2012 highlighted the power of deep learning
* effrey Dean and Andrew Ng were doing breakthrough work on large-scale image recognition 
* Richard Sutton 

> wiki timeline of ml [link](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)

> important people

* Donald Knuth

> Convolutional neurol networks <br>
> not fully connected via performance purposes <br>

* images 3 dimensional matrix
* height width length depth 
* depth is color image in 3D

> Padding

> Convolution

> bias 

> flatten

# Algorithm

* support vector machine SVM
* neurol network
* loss function

#
[Top ten algorithm for ML]([https://link](https://towardsdatascience.com/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11))

> no free lunch theorem => [website]([https://link](http://www.no-free-lunch.org/))<br>
> their is no algorithm for all the problems in the world

    Machine learning algorithms are described as learning a target function (f) that best maps input variables (X) to an output variable (Y): Y = f(X)

Historical perspective and objectives of this series
Even though deep learning made it only recently to the mainstream media, its history dates back to the early 1940’s with the first mathematical model of an artificial neuron by McCulloch & Pitts. Since then, numerous architectures have been proposed in the scientific literature, from the single layer perceptron of Frank Rosenblatt (1958) to the recent neural ordinary differential equations (2018), in order to tackle various tasks (e.g. playing Go, time-series prediction, image classification, pattern extraction, etc). The timeline below (courtesy of Favio Vázquez) provides a fairly accurate picture of deep learning’s history.

> Sources

* https://towardsdatascience.com/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11
* http://www.no-free-lunch.org/
* http://www.no-free-lunch.org/coev.pdf